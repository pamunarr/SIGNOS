{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21748323-f4c2-4376-8314-9a5b8b477032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34b84c5-b5e2-4792-8794-b19fcdc7b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "img_path = \"mano.jpeg\"\n",
    "img = plt.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7318adb-826d-44e6-b216-242e65c628f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_holistic.Holistic(static_image_mode=True) as holistic:\n",
    "    results = holistic.process(img)\n",
    "    annotated_img = img.copy()\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_img,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_hands.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "        connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1013a5-74b1-4c28-9b1f-825329666b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        ...,\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196]],\n",
       "\n",
       "       [[211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        ...,\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196]],\n",
       "\n",
       "       [[211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        [211, 216, 210],\n",
       "        ...,\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196],\n",
       "        [195, 200, 196]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 94, 108, 109],\n",
       "        [ 93, 107, 108],\n",
       "        [ 92, 106, 107],\n",
       "        ...,\n",
       "        [188, 193, 171],\n",
       "        [186, 194, 171],\n",
       "        [187, 195, 172]],\n",
       "\n",
       "       [[ 92, 106, 107],\n",
       "        [ 91, 105, 106],\n",
       "        [ 91, 105, 106],\n",
       "        ...,\n",
       "        [190, 195, 173],\n",
       "        [188, 196, 173],\n",
       "        [188, 196, 173]],\n",
       "\n",
       "       [[ 90, 104, 105],\n",
       "        [ 90, 104, 105],\n",
       "        [ 90, 104, 105],\n",
       "        ...,\n",
       "        [190, 195, 173],\n",
       "        [189, 197, 174],\n",
       "        [189, 197, 174]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(annotated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769af719-4b44-4395-9247-0729ea21508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_setting(letter, out_video_path, model_complexity, smooth_landmarks, refine_face_landmarks,\n",
    "                min_detection_confidence, min_tracking_confidence):\n",
    "    \"\"\"\n",
    "    Just for choosing good settings for detecting landmarks.\n",
    "    \"\"\"\n",
    "    # Initialize MediaPipe Solutions\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    \n",
    "    # Upload the video\n",
    "    video_path = \"../Datos/Brutos/Letras_Sematos/%s.mp4\" % letter\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video is uploaded\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Problem uploading the video.\")\n",
    "\n",
    "    # Initialize output video\n",
    "    out = cv2.VideoWriter(\n",
    "        filename=out_video_path,\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps=cap.get(cv2.CAP_PROP_FPS),\n",
    "        frameSize=(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    )\n",
    "    \n",
    "    # Configure MediaPipe Holistic Landmaker\n",
    "    with mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=model_complexity,\n",
    "        smooth_landmarks=smooth_landmarks,\n",
    "        enable_segmentation=False,\n",
    "        refine_face_landmarks=refine_face_landmarks,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence\n",
    "    ) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Take a frame of the video\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # Check if the frame is read\n",
    "            if not ret:\n",
    "                # If not, finish the process\n",
    "                break\n",
    "\n",
    "            # Detect landmarks (OpenCV works in BGR, while the landmaker in RGB)\n",
    "            results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Create the frame where landmarks will be drawn\n",
    "            annotated_frame = frame.copy()\n",
    "\n",
    "            # Draw pose, left and right hands, and face landmarks on the image.\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "\n",
    "            # Include the annotated frame in the output video\n",
    "            out.write(annotated_frame)\n",
    "    \n",
    "    # Release resources\n",
    "    out.release()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1f0ea3-3cd2-4fbb-9d94-367c30f03054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 0\n",
      "Option 1\n",
      "Option 2\n",
      "Option 3\n",
      "Option 4\n",
      "Option 5\n",
      "Option 6\n",
      "Option 7\n",
      "Option 8\n"
     ]
    }
   ],
   "source": [
    "options = [(0, 0.5), (0, 0.7), (0, 0.9), (1, 0.5), (1, 0.7), (1, 0.9), (2, 0.5), (2, 0.7), (2, 0.9)]\n",
    "for i, (o1, o2) in enumerate(options):\n",
    "    print(\"Option %s\" % i)\n",
    "    try_setting(\n",
    "        letter=\"A\",\n",
    "        out_video_path=\"../Datos/Procesados/Pruebas_A/prueba%s_A.mp4\" % i,\n",
    "        model_complexity=o1,\n",
    "        smooth_landmarks=True,\n",
    "        refine_face_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=o2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc8d13-87ef-441e-97f8-521ebd4d106a",
   "metadata": {},
   "source": [
    "Revisando visualmente los resultados, elijo (1, 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6af9e56-ea2f-40b8-a285-bc67ad7f2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skeleton(video_path):\n",
    "    # Initialize MediaPipe Holistic Landmarker\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    \n",
    "    # Upload the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video is uploaded\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Problem uploading the video.\")\n",
    "\n",
    "    # Extract info of the video\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Initialize results' list\n",
    "    vid_results = []\n",
    "    \n",
    "    # Configure MediaPipe Holistic Landmaker\n",
    "    with mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        enable_segmentation=False,\n",
    "        refine_face_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.9\n",
    "    ) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Take a frame of the video\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # Check if the frame is read\n",
    "            if not ret:\n",
    "                # If not, finish the process\n",
    "                break\n",
    "\n",
    "            # Detect landmarks (OpenCV works in BGR, while the landmaker in RGB)\n",
    "            results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Add results to the list\n",
    "            vid_results.append(results)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "\n",
    "    # Return info of the video and results' list\n",
    "    return fps, frame_width, frame_height, vid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ca3cc15-f1ef-4754-a386-3a632e2ef618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_collection_dict_landmarks(list_video_paths, list_video_abbreviations, result_path):\n",
    "    \"\"\"\n",
    "    Given a list of paths to videos, a list of the same length of the abbreviations of each video\n",
    "    and a path for storing the result, this function stores in the result_path a dictionary with\n",
    "    the Holistic Landmarks of each video.\n",
    "    The structure of the dictionary is the following:\n",
    "      - The keys of the dict are the abbreviations of the videos.\n",
    "      - Each key has a dictionary associated, whose keys are:\n",
    "        - n_frames (int): number of frames of the video.\n",
    "        - fps (int): frames per second of the video.\n",
    "        - frame_width (int): the width of the frame in pixels.\n",
    "        - frame_height (int): the height of the frame in pixels.\n",
    "        - landmarks (dict):\n",
    "          - face (ndarray): 3D array where each row is the face landmark list of a frame.\n",
    "          - pose (ndarray): 3D array where each row is the pose landmark list of a frame.\n",
    "          - pose_world (ndarray): 3D array where each row is the pose world landmark list of a frame.\n",
    "          - left_hand (ndarray): 3D array where each row is the left hand landmark list of a frame.\n",
    "          - right_hand (ndarray): 3D array where each row is the right hand landmark list of a frame.\n",
    "    \"\"\"\n",
    "    total_results = {}\n",
    "    for video_path, video_abbreviation in zip(list_video_paths, list_video_abbreviations):\n",
    "        fps, frame_width, frame_height, vid_results = generate_skeleton(video_path)\n",
    "        n_frames = len(vid_results)\n",
    "        \n",
    "        vid_dict = {\n",
    "            \"n_frames\": n_frames,\n",
    "            \"fps\": fps,\n",
    "            \"frame_width\": frame_width,\n",
    "            \"frame_height\": frame_height\n",
    "        }\n",
    "    \n",
    "        face_array = np.empty((n_frames, 478, 3), dtype=np.float64)\n",
    "        pose_array = np.empty((n_frames, 33, 4), dtype=np.float64)\n",
    "        pose_world_array = pose_array.copy()\n",
    "        left_hand_array = np.empty((n_frames, 21, 3), dtype=np.float64)\n",
    "        right_hand_array = left_hand_array.copy()\n",
    "    \n",
    "        for i, results in enumerate(vid_results):\n",
    "            face_array[i] = (list(map(lambda landmark :\n",
    "                                     list(map(lambda pos : pos[1], landmark.ListFields())),\n",
    "                                     results.face_landmarks.ListFields()[0][1]))\n",
    "                             if results.face_landmarks else np.nan)\n",
    "            \n",
    "            pose_array[i] = (list(map(lambda landmark :\n",
    "                                     list(map(lambda pos : pos[1], landmark.ListFields())),\n",
    "                                     results.pose_landmarks.ListFields()[0][1]))\n",
    "                             if results.pose_landmarks else np.nan)\n",
    "\n",
    "            pose_world_array[i] = (list(map(lambda landmark :\n",
    "                                     list(map(lambda pos : pos[1], landmark.ListFields())),\n",
    "                                     results.pose_world_landmarks.ListFields()[0][1]))\n",
    "                             if results.pose_landmarks else np.nan)\n",
    "    \n",
    "            left_hand_array[i] = (list(map(lambda landmark :\n",
    "                                          list(map(lambda pos : pos[1], landmark.ListFields())),\n",
    "                                          results.left_hand_landmarks.ListFields()[0][1]))\n",
    "                                  if results.left_hand_landmarks else np.nan)\n",
    "    \n",
    "            right_hand_array[i] = (list(map(lambda landmark :\n",
    "                                           list(map(lambda pos : pos[1], landmark.ListFields())),\n",
    "                                           results.right_hand_landmarks.ListFields()[0][1]))\n",
    "                                   if results.right_hand_landmarks else np.nan)\n",
    "        \n",
    "        landmarks = {\n",
    "            \"face\": face_array,\n",
    "            \"pose\": pose_array,\n",
    "            \"pose_world\": pose_world_array,\n",
    "            \"left_hand\": left_hand_array,\n",
    "            \"right_hand\": right_hand_array\n",
    "        }\n",
    "    \n",
    "        vid_dict[\"landmarks\"] = landmarks\n",
    "        \n",
    "        total_results[video_abbreviation] = vid_dict\n",
    "        \n",
    "    with open(result_path, \"wb\") as f:\n",
    "        dump(total_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e0e694-52d8-4c28-9f48-bd7f83f22c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [\n",
    "    'A', 'B', 'C', 'CH', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'LL', 'M',\n",
    "    'N', 'N_', 'O', 'P', 'Q', 'R', 'RR', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'\n",
    "]\n",
    "list_video_paths = [\"../Datos/Brutos/Letras_Spread/%s.mp4\" % letter for letter in alphabet]\n",
    "result_path = \"../Datos/Procesados/alphabet_landmarks_spread.pkl\"\n",
    "\n",
    "obtain_collection_dict_landmarks(list_video_paths, alphabet, result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
